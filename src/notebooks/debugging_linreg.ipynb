{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "400f742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: c:\\Users\\bengu\\Documents\\NFL Data Project\\clairvoyent-raven-sports-analysis\\src\n",
      "Sys Path Before: ['c:\\\\Users\\\\bengu\\\\Documents\\\\NFL Data Project\\\\clairvoyent-raven-sports-analysis\\\\src', 'C:\\\\Users\\\\bengu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\python310.zip', 'C:\\\\Users\\\\bengu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\DLLs', 'C:\\\\Users\\\\bengu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib', 'C:\\\\Users\\\\bengu\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310', 'c:\\\\Users\\\\bengu\\\\.virtualenvs\\\\cfeproj-oIABPDjj', '', 'c:\\\\Users\\\\bengu\\\\.virtualenvs\\\\cfeproj-oIABPDjj\\\\lib\\\\site-packages', 'c:\\\\Users\\\\bengu\\\\.virtualenvs\\\\cfeproj-oIABPDjj\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\bengu\\\\.virtualenvs\\\\cfeproj-oIABPDjj\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\bengu\\\\.virtualenvs\\\\cfeproj-oIABPDjj\\\\lib\\\\site-packages\\\\Pythonwin']\n",
      "Loading base data frames...\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Project path setup\n",
    "# -----------------------------------------------------------------------------\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(\"Sys Path Before:\", sys.path)\n",
    "if project_root not in sys.path:\n",
    "    print(\"Inserting project root to sys.path\")\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Now import internal modules\n",
    "import utils\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Constants / Config\n",
    "# -----------------------------------------------------------------------------\n",
    "COLUMN_CATEGORIES = utils.STATISTICAL_COLUMNS_BY_CATEGORY\n",
    "TARGET_INPUTS = utils.TARGETS_TO_INPUTS\n",
    "\n",
    "ROLLING_PERIOD = 4\n",
    "\n",
    "CATEGORIES_POSITIONS = {\n",
    "    \"passing\": [\"QB\"],\n",
    "    \"rushing_and_receiving\": [\"RB\", \"WR\", \"TE\", \"QB\"],\n",
    "    # (kicking not available)\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load Persistent DataFrames\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Loading base data frames...\")\n",
    "player_stats_path = r\"C:\\Users\\bengu\\Documents\\NFL Data Project\\clairvoyent-raven-sports-analysis\\data\\scrubbed_player_data.xlsx\" # os.getenv(\"NFLVERSE_DATA_PATH\")\n",
    "teams_stats_path = r\"C:\\Users\\bengu\\Documents\\NFL Data Project\\clairvoyent-raven-sports-analysis\\data\\team_stats.csv\" # os.getenv(\"NFLVERSE_TEAMS_DATA_PATH\")\n",
    "injuries_path = os.getenv(\"NFLVERSE_INJURIES_PATH\")\n",
    "depth_path = os.getenv(\"NFLVERSE_DEPTH_CHART_PATH\")\n",
    "\n",
    "all_players_df = pd.read_excel(rf\"{player_stats_path}\", engine=\"openpyxl\")\n",
    "all_teams_df = pd.read_csv(rf\"{teams_stats_path}\")\n",
    "injuries_df = pd.read_excel(rf\"{injuries_path}\", engine=\"openpyxl\")\n",
    "depth_df = pd.read_excel(rf\"{depth_path}\", engine=\"openpyxl\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e95aa759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def encode_and_filter_injuries_data(injuries_df: pd.DataFrame = injuries_df):\n",
    "    df_inj = injuries_df.copy()\n",
    "    if \"gsis_id\" in df_inj.columns:\n",
    "        df_inj = df_inj.rename({\"gsis_id\": \"player_id\"}, axis=1)\n",
    "\n",
    "    filtered = df_inj[[\"season\", \"week\", \"player_id\", \"report_status\", \"practice_status\"]].copy()\n",
    "\n",
    "    encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    X_cat = filtered[[\"report_status\", \"practice_status\"]]\n",
    "    encoded_data = encoder.fit_transform(X_cat)\n",
    "    encoded_feature_names = encoder.get_feature_names_out(X_cat.columns)\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names, index=filtered.index).fillna(0)\n",
    "\n",
    "    out = pd.concat([filtered[[\"season\", \"week\", \"player_id\"]], encoded_df], axis=1)\n",
    "    return out, encoded_feature_names\n",
    "\n",
    "\n",
    "def filter_depth_data(depth_df: pd.DataFrame = depth_df):\n",
    "    # Always start from a concrete base df\n",
    "    if \"gsis_id\" in depth_df.columns:\n",
    "        base = depth_df.rename({\"gsis_id\": \"player_id\"}, axis=1).copy()\n",
    "    else:\n",
    "        base = depth_df.copy()\n",
    "\n",
    "    filtered = base[[\"season\", \"week\", \"player_id\", \"depth_team\"]].copy()\n",
    "    filtered = filtered.dropna(subset=[\"season\", \"week\"]).reset_index(drop=True)\n",
    "\n",
    "    filtered[\"week\"] = filtered[\"week\"].astype(int)\n",
    "    filtered[\"season\"] = filtered[\"season\"].astype(int)\n",
    "\n",
    "    # Fill NaNs in depth with mean depth (around 1.5/2.0)\n",
    "    if filtered[\"depth_team\"].isna().any():\n",
    "        filtered[\"depth_team\"] = filtered[\"depth_team\"].fillna(filtered[\"depth_team\"].mean())\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def merge_players_to_depth_and_injury(\n",
    "    all_players_df: pd.DataFrame,\n",
    "    filtered_injuries_df: pd.DataFrame,\n",
    "    filtered_depth_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    df = all_players_df.merge(filtered_injuries_df, how=\"left\", on=[\"player_id\", \"season\", \"week\"])\n",
    "    df = df.merge(filtered_depth_df, how=\"left\", on=[\"player_id\", \"season\", \"week\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_by_positional_group(df: pd.DataFrame, category_key_a: str, category_key_b: str = \"standard\") -> pd.DataFrame:\n",
    "    new_df = df.loc[:, list(COLUMN_CATEGORIES[category_key_a] | COLUMN_CATEGORIES[category_key_b])].copy()\n",
    "    new_df = new_df[new_df[\"position\"].isin(CATEGORIES_POSITIONS[category_key_a])]\n",
    "    return new_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def calculate_rolling_data(\n",
    "    df: pd.DataFrame,\n",
    "    sort_values: list,\n",
    "    input_ref: str,\n",
    "    groupby: list,\n",
    "    rolling_period: int = 3,\n",
    "    min_periods: int = 1,\n",
    "    shift: int = 1,\n",
    ") -> pd.DataFrame:\n",
    "    df = df.sort_values(sort_values)\n",
    "    cols = TARGET_INPUTS[input_ref]\n",
    "    df[[f\"{c}_roll{rolling_period}_shift\" for c in cols]] = (\n",
    "        df.groupby(groupby)[cols]\n",
    "          .transform(lambda x: x.rolling(rolling_period, min_periods=min_periods).mean().shift(shift))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_cumulative_data(\n",
    "    df: pd.DataFrame,\n",
    "    sort_values: list,\n",
    "    input_ref: str,\n",
    "    groupby: list,\n",
    "    prefix: str = \"\",\n",
    "    shift: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    df = df.sort_values(sort_values)\n",
    "    cols = TARGET_INPUTS[input_ref]\n",
    "    # mean\n",
    "    df[[f\"{prefix}{c}_cum_avg\" for c in cols]] = (\n",
    "        df.groupby(groupby)[cols]\n",
    "          .transform(lambda x: x.expanding().mean().shift(shift))\n",
    "    )\n",
    "    # std (sample std by default); adjust min_periods/ddof if you prefer\n",
    "    df[[f\"{prefix}{c}_cum_std\" for c in cols]] = (\n",
    "        df.groupby(groupby)[cols]\n",
    "          .transform(lambda x: x.expanding().std().shift(shift))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_standard_input_cols(target: str, encoded_feature_names) -> list[str]:\n",
    "    rolling_cols = [col + f\"_roll{ROLLING_PERIOD}_shift\" for col in TARGET_INPUTS[target]]\n",
    "    avg_cum_cols = [col + \"_cum_avg\" for col in TARGET_INPUTS[target]]\n",
    "    std_cum_cols = [col + \"_cum_std\" for col in TARGET_INPUTS[target]]\n",
    "    opp_avg_cum_cols = [\"vs_opponent_\" + col + \"_cum_avg\" for col in TARGET_INPUTS[target]]\n",
    "    opp_std_cum_cols = [\"vs_opponent_\" + col + \"_cum_std\" for col in TARGET_INPUTS[target]]\n",
    "\n",
    "    return (\n",
    "        rolling_cols\n",
    "        + avg_cum_cols\n",
    "        + std_cum_cols\n",
    "        + opp_avg_cum_cols\n",
    "        + opp_std_cum_cols\n",
    "        + [\"depth_team\"]\n",
    "        + list(encoded_feature_names)\n",
    "    )\n",
    "\n",
    "\n",
    "def scale_inplace(df: pd.DataFrame, cols: list[str], name: str, scalers: dict | None = None):\n",
    "    if scalers is None:\n",
    "        scalers = {}\n",
    "    scaler = StandardScaler()\n",
    "    df.loc[:, cols] = scaler.fit_transform(df[cols])\n",
    "    scalers[name] = scaler\n",
    "    return df, scalers\n",
    "\n",
    "\n",
    "def handle_merge(\n",
    "    left_df: pd.DataFrame,\n",
    "    right_df: pd.DataFrame,\n",
    "    how: str = \"left\",\n",
    "    left_on: list = [\"opponent_team\", \"season\", \"week\"],\n",
    "    right_on: list = [\"team\", \"season\", \"week\"],\n",
    ") -> pd.DataFrame:\n",
    "    return left_df.merge(right_df, how=how, left_on=left_on, right_on=right_on)\n",
    "\n",
    "\n",
    "def generate_target_dataframe_struct(\n",
    "    encoded_feature_names,\n",
    "    rushing_and_receiving_df: pd.DataFrame,\n",
    "    passing_df: pd.DataFrame,\n",
    "    all_teams_df: pd.DataFrame,\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    standard_inputs = [\n",
    "        \"season\", \"week\", \"player_id\", \"position\",\n",
    "        \"player_name\", \"team\", \"opponent_team\", \"depth_team\",\n",
    "    ]\n",
    "    enc = list(encoded_feature_names)\n",
    "\n",
    "    target_data_struct: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    rushing_cats = [\"rsh_yd\", \"rsh_td\", \"rc_yd\", \"rc_td\", \"rc\"]\n",
    "    passing_cats = [\"p_yd\", \"p_td\", \"intcpt\"]\n",
    "    both_cats = [\"rsh_fmbls\", \"rc_fmbls\"]\n",
    "\n",
    "    for target in TARGET_INPUTS:\n",
    "        if target in rushing_cats:\n",
    "            target_data_struct[target] = rushing_and_receiving_df[TARGET_INPUTS[target] + standard_inputs + enc].copy()\n",
    "        elif target in passing_cats:\n",
    "            target_data_struct[target] = passing_df[TARGET_INPUTS[target] + standard_inputs + enc].copy()\n",
    "        elif target in both_cats:\n",
    "            both = pd.concat([rushing_and_receiving_df, passing_df], ignore_index=True)\n",
    "            target_data_struct[target] = both[TARGET_INPUTS[target] + standard_inputs + enc].copy()\n",
    "\n",
    "    # Defensive (opponent) stats handled separately\n",
    "    target_data_struct[\"def\"] = all_teams_df[TARGET_INPUTS[\"def\"] + [\"season\", \"week\", \"team\"]].copy()\n",
    "\n",
    "    return target_data_struct\n",
    "\n",
    "\n",
    "def get_input_cols_by_target(target_data_struct: dict[str, pd.DataFrame], encoded_feature_names) -> dict[str, list[str]]:\n",
    "    input_cols_by_target: dict[str, list[str]] = {}\n",
    "    for target in target_data_struct:\n",
    "        if target == \"def\":\n",
    "            input_cols_by_target[target] = [col + f\"_roll{ROLLING_PERIOD}_shift\" for col in TARGET_INPUTS[target]]\n",
    "        else:\n",
    "            input_cols_by_target[target] = get_standard_input_cols(target, encoded_feature_names)\n",
    "    return input_cols_by_target\n",
    "\n",
    "\n",
    "def calculate_rolling_and_cumulative_data(\n",
    "    target_data_struct: dict[str, pd.DataFrame],\n",
    "    rolling_period: int = ROLLING_PERIOD,\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    for target, df in target_data_struct.items():\n",
    "        if target == \"def\":\n",
    "            tmp = calculate_rolling_data(df, [\"season\", \"week\", \"team\"], target, [\"season\", \"team\"], rolling_period=rolling_period)\n",
    "            tmp = calculate_cumulative_data(tmp, [\"season\", \"week\", \"team\"], target, [\"season\", \"team\"])\n",
    "            target_data_struct[target] = tmp\n",
    "        else:\n",
    "            tmp = calculate_rolling_data(df, [\"season\", \"week\", \"player_id\"], target, [\"season\", \"player_id\"], rolling_period=rolling_period)\n",
    "            tmp = calculate_cumulative_data(tmp, [\"season\", \"week\", \"player_id\"], target, [\"season\", \"player_id\"])\n",
    "            tmp = calculate_cumulative_data(tmp, [\"season\", \"week\", \"player_id\"], target, [\"opponent_team\", \"player_id\"], prefix=\"vs_opponent_\")\n",
    "            target_data_struct[target] = tmp\n",
    "    return target_data_struct\n",
    "\n",
    "\n",
    "def scale_target_data(target_data_struct: dict[str, pd.DataFrame], target_input_cols: dict[str, list[str]]) -> dict[str, pd.DataFrame]:\n",
    "    for target, df in target_data_struct.items():\n",
    "        cols = target_input_cols.get(target, [])\n",
    "        if not cols:\n",
    "            continue\n",
    "        target_data_struct[target], _ = scale_inplace(df, cols, target)  # discard scalers for now\n",
    "    return target_data_struct\n",
    "\n",
    "\n",
    "def merge_target_data_to_defense(target_data_struct: dict[str, pd.DataFrame]) -> dict[str, pd.DataFrame]:\n",
    "    # merge 'def' onto each offensive df\n",
    "    def_df = target_data_struct[\"def\"]\n",
    "    for target, df in target_data_struct.items():\n",
    "        if target == \"def\":\n",
    "            continue\n",
    "        target_data_struct[target] = handle_merge(df, def_df)\n",
    "    return target_data_struct\n",
    "\n",
    "def train_and_validate_model(\n",
    "    target_data_struct: dict[str, pd.DataFrame],\n",
    "    target_input_cols: dict[str, list[str]],\n",
    "    season_holdout: int = 2024,\n",
    "):\n",
    "    model_results: dict[str, dict] = {}\n",
    "    models: dict[str, LinearRegression] = {}\n",
    "\n",
    "    for target, df in target_data_struct.items():\n",
    "        if target == \"def\":\n",
    "            continue  # no target variable for defense\n",
    "\n",
    "        input_cols = target_input_cols[target]\n",
    "        def_input_cols = target_input_cols[\"def\"]\n",
    "\n",
    "        # hold out a season for validation split (and avoid leakage)\n",
    "        mask = df[\"season\"] != season_holdout\n",
    "\n",
    "        # Build a single feature matrix with aligned rows, then dropna once\n",
    "        feature_cols = input_cols + def_input_cols\n",
    "        XY = df.loc[mask, feature_cols + [target]].dropna().reset_index(drop=True)\n",
    "\n",
    "        if XY.empty:\n",
    "            model_results[target] = {\"validation_rmse\": \"nan\", \"r2\": \"nan\"}\n",
    "            continue\n",
    "\n",
    "        X = XY[feature_cols].values\n",
    "        y = XY[target].values\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        reg = LinearRegression().fit(X_train, y_train)\n",
    "        preds = reg.predict(X_valid)\n",
    "\n",
    "        rmse = root_mean_squared_error(y_valid, preds)\n",
    "        r2 = r2_score(y_valid, preds)\n",
    "\n",
    "        model_results[target] = {\"validation_rmse\": f\"{rmse:.4f}\", \"r2\": f\"{r2:.3f}\"}\n",
    "        models[target] = reg\n",
    "\n",
    "    return models, model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89fb19a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering and merging injuries and depth charts...\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Prepare injuries/depth and merge with players first\n",
    "print(\"Filtering and merging injuries and depth charts...\")\n",
    "filtered_injuries_df, encoded_feature_names = encode_and_filter_injuries_data()\n",
    "filtered_depth_df = filter_depth_data()\n",
    "merged_players = merge_players_to_depth_and_injury(all_players_df, filtered_injuries_df, filtered_depth_df)\n",
    "print(\"-\" * 40)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d62e51ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating positional dataframes...\n",
      "Generated positional dataframes with 836418 rows and 8713458 rows\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) Build positional dataframes FROM the merged players df\n",
    "print(\"Generating positional dataframes...\")\n",
    "passing_df = filter_by_positional_group(merged_players, \"passing\")\n",
    "rushing_and_receiving_df = filter_by_positional_group(merged_players, \"rushing_and_receiving\")\n",
    "print(f\"Generated positional dataframes with {passing_df.size} rows and {rushing_and_receiving_df.size} rows\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Optional: quick null check utility (add .alias only if used)\n",
    "passing_df.alias = \"passing_df\"\n",
    "rushing_and_receiving_df.alias = \"rushing_and_receiving_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12c556c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating target data structure and input columns by statistical category...\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Create per-target dataframes\n",
    "print(\"Generating target data structure and input columns by statistical category...\")\n",
    "target_data_struct = generate_target_dataframe_struct(\n",
    "    encoded_feature_names, rushing_and_receiving_df, passing_df, all_teams_df\n",
    ")\n",
    "target_input_cols = get_input_cols_by_target(target_data_struct, encoded_feature_names)\n",
    "print(\"-\" * 40)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "73f476c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for cumulative and rolling data...\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Feature engineering: rolling / cumulative (season & vs-opponent), then scale, then merge defense\n",
    "print(\"Engineering features for cumulative and rolling data...\")\n",
    "target_data_struct = calculate_rolling_and_cumulative_data(target_data_struct)\n",
    "target_data_struct = scale_target_data(target_data_struct, target_input_cols)\n",
    "target_data_struct = merge_target_data_to_defense(target_data_struct)\n",
    "print(\"-\" * 40)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cbb87545",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_translation = {\n",
    "           \"rsh_yd\": \"rushing_yards\", \n",
    "           \"rsh_td\": \"rushing_tds\", \n",
    "           \"rc_yd\": \"receiving_yards\", \n",
    "           \"rc_td\": \"receiving_tds\", \n",
    "           \"rc\": \"receptions\", \n",
    "           \"p_yd\": \"passing_yards\", \n",
    "           \"p_td\": \"passing_tds\", \n",
    "           \"intcpt\": \"passing_interceptions\", \n",
    "           \"rsh_fmbls\": \"rushing_fumbles_lost\", \n",
    "           \"rc_fmbls\": \"receiving_fumbles_lost\"\n",
    "}\n",
    "\n",
    "def train_and_validate_model(\n",
    "    target_data_struct: dict[str, pd.DataFrame],\n",
    "    target_input_cols: dict[str, list[str]],\n",
    "    season_holdout: int = 2024,\n",
    "):\n",
    "    model_results: dict[str, dict] = {}\n",
    "    models: dict[str, LinearRegression] = {}\n",
    "    predictions: dict[str, np.array] = {}\n",
    "    trues: dict[str, np.array] = {}\n",
    "\n",
    "    for target, df in target_data_struct.items():\n",
    "        if target == \"def\":\n",
    "            continue  # no target variable for defense\n",
    "\n",
    "        input_cols = target_input_cols[target]\n",
    "        def_input_cols = target_input_cols[\"def\"]\n",
    "\n",
    "        # hold out a season for validation split (and avoid leakage)\n",
    "        mask = df[\"season\"] != season_holdout\n",
    "\n",
    "        # Build a single feature matrix with aligned rows, then dropna once\n",
    "        feature_cols = input_cols + def_input_cols\n",
    "        XY = df.loc[mask, feature_cols + [target_translation[target]]].fillna(0) #.dropna().reset_index(drop=True)\n",
    "\n",
    "        if XY.empty:\n",
    "            model_results[target] = {\"validation_rmse\": \"nan\", \"r2\": \"nan\"}\n",
    "            continue\n",
    "\n",
    "        X = XY[feature_cols].values\n",
    "        y = XY[target_translation[target]].values\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        reg = LinearRegression().fit(X_train, y_train)\n",
    "        preds = reg.predict(X_valid)\n",
    "\n",
    "        rmse = root_mean_squared_error(y_valid, preds)\n",
    "        r2 = r2_score(y_valid, preds)\n",
    "\n",
    "        model_results[target] = {\"validation_rmse\": f\"{rmse:.4f}\", \"r2\": f\"{r2:.3f}\"}\n",
    "        models[target] = reg\n",
    "        predictions[target] = preds\n",
    "        trues[target] = y_valid\n",
    "\n",
    "    return models, model_results, trues, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "70af8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = target_data_struct[\"p_yd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d5d06b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model...\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "MODEL RESULTS:\n",
      "rsh_yd: {'validation_rmse': '15.6698', 'r2': '0.581'}\n",
      "rsh_td: {'validation_rmse': '0.2668', 'r2': '0.205'}\n",
      "rsh_fmbls: {'validation_rmse': '0.1341', 'r2': '0.122'}\n",
      "rc_yd: {'validation_rmse': '23.0212', 'r2': '0.449'}\n",
      "rc_td: {'validation_rmse': '0.3577', 'r2': '0.161'}\n",
      "rc: {'validation_rmse': '1.6301', 'r2': '0.486'}\n",
      "rc_fmbls: {'validation_rmse': '0.1036', 'r2': '0.051'}\n",
      "p_yd: {'validation_rmse': '308.0146', 'r2': '-7.710'}\n",
      "p_td: {'validation_rmse': '2.8312', 'r2': '-5.348'}\n",
      "intcpt: {'validation_rmse': '1.1975', 'r2': '-0.625'}\n"
     ]
    }
   ],
   "source": [
    "# 5) Train & validate models\n",
    "print(\"Training and validating model...\")\n",
    "models, model_results, trues, preds = train_and_validate_model(target_data_struct, target_input_cols, season_holdout=2025)\n",
    "print(\"-\" * 40) \n",
    "print(\"\\n\")\n",
    "  \n",
    "# 6) Report\n",
    "print(\"\\nMODEL RESULTS:\")\n",
    "for target, metrics in model_results.items():\n",
    "    print(f\"{target}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0841dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_store_model_weights(models: dict, path: str = \"models\"):\n",
    "    \"\"\"Save the coefficients and intercept for the linear regression models to models/ folder.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    for name, lr in models.items():\n",
    "        np.savez(\n",
    "            os.path.join(path, f\"{name}_linreg_weights.npz\"),\n",
    "            coef=lr.coef_,\n",
    "            intercept=lr.intercept_,\n",
    "            n_features_in_=lr.n_features_in_,\n",
    "            feature_names_in_=getattr(lr, \"feature_names_in_\", None),\n",
    "        )\n",
    "        \n",
    "save_and_store_model_weights(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfeproj-oIABPDjj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
